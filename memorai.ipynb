{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Open the webcam (use 0 for the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Draw rectangles around detected faces and label them as \"Unknown\"\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "        # Label the face as \"Unknown\"\n",
    "        cv2.putText(frame, 'Unknown', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732806139.926541 40795586 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1732806139.945866 40796110 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732806139.952176 40796110 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1732806141.264955 40796108 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh()\n",
    "\n",
    "# Set the font path for modern text rendering (update this for your OS)\n",
    "font_path = \"Roboto-Light.ttf\"  # Update with your font path\n",
    "font = ImageFont.truetype(font_path, 24)  # Font size 24 for modern look\n",
    "\n",
    "# Load a small background image for the text\n",
    "text_background_path = \"background.png\"  # Path to your text background image\n",
    "text_background = Image.open(text_background_path)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to RGB for MediaPipe processing\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame for facial landmarks\n",
    "    result = face_mesh.process(rgb_frame)\n",
    "\n",
    "    # Convert the frame to a Pillow image for rendering modern text\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    # Draw landmarks and names\n",
    "    if result.multi_face_landmarks:\n",
    "        for face_landmarks in result.multi_face_landmarks:\n",
    "            # Get approximate top of the face for text placement\n",
    "            top_x = int(face_landmarks.landmark[10].x * frame.shape[1])  # Landmark near forehead\n",
    "            top_y = int(face_landmarks.landmark[10].y * frame.shape[0]) - 30  # Place text above forehead\n",
    "\n",
    "            # Draw landmarks using OpenCV (retain blue points)\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                x = int(landmark.x * frame.shape[1])\n",
    "                y = int(landmark.y * frame.shape[0])\n",
    "                cv2.circle(frame, (x, y), 1, (255, 0, 0), -1)  # Blue points\n",
    "\n",
    "            # Resize and paste the text background image\n",
    "            bg_width, bg_height = 300, 300  # Adjust size for your text background\n",
    "            resized_bg = text_background.resize((bg_width, bg_height))\n",
    "            pil_image.paste(resized_bg, (top_x - 150, top_y - 20), resized_bg)  # Transparent paste (RGBA)\n",
    "\n",
    "            # Add name and subtitle on top of the background\n",
    "            draw.text((top_x - 150, top_y + 290), \"MOHAMMED S. ALZIYAD\", font=font, fill=(255, 255, 255))  # Name in blue\n",
    "            draw.text((top_x - 150, top_y + 310), \"Your best friend\", font=font, fill=(255, 255, 255))  # Subtitle\n",
    "\n",
    "    # Convert the updated Pillow image back to OpenCV format\n",
    "    frame_with_text_bg = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Face Mesh with Text Background\", frame_with_text_bg)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
